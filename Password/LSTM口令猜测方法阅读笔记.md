---
sort: 3
---



# LSTM口令猜测方法论文阅读笔记

题目：Fast,lean and Accurate: Modeling Password Guessability Using Neural Networks

时间：2020年11月22日

## 概述

提出用人工神经网络模拟文本密码抵抗猜测攻击的能力。神经网络的方法比现有的方法更加高效。更重要的是，神经网络模型能够被压缩而不影响效率，增加口令检查的实用性。



## 引入

1）口令评估强度的重要性。

2）研究全面测试了不同的神经网络模型大小、模型架构、训练数据和训练技术对网络猜测不同类型密码能力的影响。

3）由于神经网络模型能够被有效压缩，神经网络方法更加适用于客户端口令猜测

4）实现用于客户端口令强度评估的检查器



## 神经网络方法

虽然密码创建和文本生成在概念上是相似的，但很少有研究尝试使用文本生成的见解来建模密码。

从概念上讲，神经网络比其他方法有优势。与PCFGs和Markov模型相比，神经网络生成的序列可能是不精确的，这点反应神经网络可能适合猜测密码。概率密码猜测方法占用了大量内存，因此仅在客户端是不切实际的。然而，神经网络建模自然语言的空间比马尔科夫模型小得多。



## LSTM系统设计

与马尔可夫模型相似，神经网络在LSTM系统中被训练成根据密码前面的字符生成密码的下一个字符。![image-20201129193310300](assets\image-20201129193310300.png)

LSTM的猜测枚举方法类似于马尔科夫模型中使用的方法，但是LSTM的方法能够从相似的优化中受益。另外，与马尔可夫模型相比，神经网络模型的一个主要优点是可以在GPU上高效地实现。

通过对猜测攻击建模来评估密码强度，获得计算密码的猜测数，或者按照可能性降序排列，即按照破解需要猜测的次数排列。

### 系统实现细节

#### 1. 模型架构

因为递归神经网络已经被证明在字符级自然语言的环境中生成文本是有用的，LSTM的实现基于现成的RNN网络。递归神经网络是一种特殊类型的神经网络，在这种网络中，网络中的连接可以按顺序处理元素，并使用内部存储器来记住序列中先前元素的信息。

#### 2. 字典大小

LSTM关注的是字符级模型，而不是更常见的单词级模型，因为没有用于生成密码的单词字典。在混合结构中，除了字符之外，神经网络还可以建模子单词单位，如音节或符号。同时建模所有字符都不必要地给模型带来负担。而一些字符，比如大写字母和稀有符号，最好在神经网络之外建模。换句话说，通过将模型的输出解释为模板，然后基于这些字符创建包含特殊字符的密码。

#### 3.  口令上下文

预测依赖于上下文字符，概念类似于MArkov的n-gram。增加上下文字符的数量会增加训练时间，而减少上下文字符的数量可能会降低猜测的成功率。我们尝试在密码中使用前面的所有字符作为上下文，只使用前面的10个字符。实践表明10个字符的效果比较优秀，所以模型采用10个字符作为上下文关系的推断依据。

#### 4.  模型大小

为了衡量改变模型大小对猜测成功的影响，实验测试了一个有15,700,675个参数的大型神经网络和一个有682,851个参数的小型神经网络。选择更大的尺寸限制的时间和GPU内存使用的模型。较小的大小选择适合浏览器实现。

#### 5. 迁移学习

迁移学习可以训练一个模型来模拟所有的密码，但只对更长的密码进行猜测。在使用迁移学习时，首先对训练集中的所有密码对模型进行训练，然后冻结模型的下层。直观上，模型中的较低层次学习数据的低级特征(例如，“a”是一个元音)，较高层学习数据的高级特征。

#### 6. 训练数据

一般来说，对于机器学习算法来说，训练数据越多越好，但前提是训练数据与测试的密码非常匹配。

## 实验

为了评估神经网络的实现，实验将其与其他多种密码破解方法进行比较，包括PCFGs、Markov模型、JtR和Hashcat。

### 训练数据

除了只需要8个字符的通用策略外，我们还研究了三种不太常见的、更能抵抗猜测的密码策略4class8、3class12和1class16。

## 评估

+ 与其他猜测密码的方法相比，在猜测次数较多的情况下，以及针对更复杂或更长的密码策略(如4class8、1class16和3class12数据集)，神经网络更擅长猜测密码。
+ MinGuess的性能优于神经网络，这表明，使用多种猜测方法来进行准确的强度估计，仍然应该首选使用任何单一的猜测方法，尽管事实上，神经网络通常比其他单独的模型表现更好。
+ 资源需求通常，PCFGs需要最多的磁盘、内存和计算资源。PCFG实现将其语法存储在4.7GB的磁盘空间中。马尔可夫模型是第二大实现，需要1.1GB的磁盘空间。Hashcat和JtR不需要为它们的规则存储大量的空间，但是需要存储整个训练集，这是756MB。相比之下，LSTM神经网络只需要60MB磁盘空间。