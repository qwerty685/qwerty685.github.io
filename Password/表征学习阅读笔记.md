---
sort: 5
---



# 表征学习的口令猜测方法阅读笔记

## 生成模型

一个深层生成模型是一个概率模型，训练它拟合未知目标数据分布的隐式估计p∗(x)。在此过程中，利用深度神经网络对底层数据分布的描述进行参数化。

生成网络是潜在空间Z: R[k]和数据空间X(即定义观测数据的空间)之间的确定性映射函数G:  Z→X，是’p(Z)和模型学到的分布p(X)之间的桥梁。更正式地说，在这种构造下，数据实例的概率有以下形式:![image-20201123151213183](assets\image-20201123151213183.png)

根据'p(z)从潜在空间中采样点z，然后通过生成器将其映射到数据空间中。这个过程等价于根据p(X)从数据空间X中采样数据点。



### A. GAN

![image-20201123150923856](assets\image-20201123150923856.png)



GANs框架通过对抗性的训练方法学习深层生成模型。训练过程由判别器D指导，它给生成网络G生成损失函数。



### B. AutoEncoder



![image-20201123150933238](assets\image-20201123150933238.png)

Autoencoder是深生成模型，概念上由两个网络组成:一个编码器网络Enc:  X→Z和译码器网络:DEC：Z→X。生成的聚合模型通常是培训学习一种恒等函数:X = DEC(Enc  (X))。



## CPG

### A. 口令强局部性

就像之前提到的，生成器的潜在表示特性增强了数据中共享语义的潜在语义的几何联系。这意味着，相似的口令具有相近的潜在表示。对于口令的潜在表示，相似性体现在口令结构、公共字符串的出现以及字符种类这些关键因素。

我们松散地将密码局部相关命名为“**强局部性**”，即将共享非常细粒度特征的密码分组在一起的表示的内在属性。

由于不同类别的密码被组织并绑定到底层空间的不同区域，因此可以通过从密码的特定区域采样来生成特定类别的密码。主要问题在于如何描述我们感兴趣的口令所在潜在空间的潜在表示。

一种简单的想法是通过引入中心点x获取x的潜在表示z。根据口令的强局部性，通过微调潜在表示z得到的潜在表示z‘能够得到与x具有强相似性的口令。换句话说，将生成器生成的口令限制在一个与z具有潜在语义相关的区域内能够获取与x强相关的口令。（潜在空间的分布使用的是高斯分布，那么控制生成范围的变量就是高斯分布的参数theta）

### B. 口令模板与局部口令生成

通过在字母中引入通配符，编码器网络可以被迫处理特定的密码定义。通配符，由符号“∗”表示，可以用作占位符，以指示未指定的字符。例如，模板“jimmy∗∗”表示一个密码类，它以字符串“jimmy”开头，后跟两个通配符。当与输入字符串对应的矩阵被提供给编码器时，我们通过将通配符映射到一个空的单热编码向量来实现这一行为。

编码器将口令模板转变为口令的潜在表示，解码器将口令的潜在表示进行微调产生对应模板的猜测口令。

### C. Conditional Password Guessing (CPG)

具有限制的口令猜测在现实中存在实用场景：

+ 攻击者可能对生成具有特定结构或公共子字符串的任意数量的猜测感兴趣
+ 攻击者可以使用能够使用部分知识的条件密码生成方法来提高以用户输入为目标的侧通道攻击的影响
+ 合法用户可能对恢复她/他忘记的密码感兴趣，用户能够回忆口令的部分模板

基于CPG的口令猜测能够将生成的口令限制在精确的潜在空间内，有效利用已有的口令信息，提升口令猜测效率。



### D. 评估

（1）创建部分字符缺失的口令模板

​	从LinkedIn数据集中选择一个口令（长度小于等于16），用通配符替代口令中的每一个字符。例如，口令“jimmy1991”变成“*i*my***1”。每个字符替换成通配符的概率为0.5，同时要求剩余的模板至少包含4个可见字符和5个通配符，保证直接暴力破解无法进行。

​	产生模板集后，对模板集进行分类。一个口令模板可能对应多条口令，口令匹配的数量可以称为模板的稀缺度。实验将模板集分成四类：常见，不常见，稀有，超稀有。分别对应以下关系：

1) T-common，匹配的口令数为[1000,15000]

2)T-uncommon，匹配的口令数为[50,150]

3)T-rare，匹配的口令数为[10,15]

4)T-super-rare，匹配的口令数为[1,5]

​	最终，实验从四类模板各取30条口令模板用于评估CPG的效率。

（2）结果

实验基于CWAE（相比GAN的实现要更加契合这个场景）。最大长度为16，原因是10-22的口令长度的效果相差不大。

实验为每个口令模板生成10^7条口令，计算这些口令与目标口令集匹配的口令集的交集。

CPG将与五种方法进行比较：OMEN，FLA，PCFG，HashCat还有CMU-PGS。使用min-auto的方法进行比较。每一种方法产生10^10条口令，从中挑选符合模板的口令。计算挑选的口令集与目标口令集符合条件的口令集的交集评估每种方法的猜测能力。

![image-20201121213257660](assets\image-20201121213257660.png)

实验结果表明，在给定口令模板的前提下，CPG拥有很高的命中率，而且，命中率与模板的稀缺程度无关。有趣的是，对于比较常见的口令模板，CPG的猜测能力比单个口令猜测方法更强。另外，由于CPG对每个模板只产生了10^7个猜测，这意味着增加猜测数将增加更高的口令命中率。



## DPG和口令弱局部相关性

### A. 口令弱局部相关性

潜空间的嵌入特性将具有相似特性的口令映射在较近的距离内，这称为口令的强局部相关性。“强”字眼强调生成口令之间严格的语义关系。然而，相同的动态特性使口令间的语义关系有更加广泛的形式，这称为口令的弱局部相关性。

用更加形象的话语表述就是，来自相同口令集的口令在潜语义空间中聚集在一起，不会突然分散到整个潜语义空间。

### B. DPG背后的理论依据以及实现

1）理论基础

基于概率的口令猜测方法都是显式或者隐式地尽可能捕捉口令集背后的数据分布。论文假设所有泄漏的数据集的分布都可以通过一个特殊的概率分布p\*(x)表示。p\*(x)有足够的表达能力表示口令分布的整体。

现实生活的口令猜测场景都是在不同口令分布的数据集上进行的。这意味着猜测者缺少带破解口令集的分布信息。训练集和测试集的这种差异在机器学习领域称为covariate shift（协变量偏移）。

为了减少协变量偏移带来的影响，猜测者能够使用已经猜测的口令评估剩余口令的分布信息。一旦破解者得到足够破解口令就能够学习待破解口令的分布信息。这个过程类似贝叶斯概率估计，因为能够在观察和猜测的过程中产生持续的反馈。

对于数据驱动模型，一个简单的实现就是在成功的猜测后对口令模型进行微调，用于改变口令模型生成口令的分布。但是直接修改模型是困难的，在猜测的过程中直接修改模型将带来巨大的计算负担，另外也无法保证最终生成结果的有效性。

对于类似GAN的生成模型，生成口令的分布是一个与p(z)有关的联合分布。p(z)是潜变量分布。

![image-20201122201801876](assets\image-20201122201801876.png)

基于生成模型的猜测器，为了修改生成器产生的口令分布，除了修改生成器的模型参数以外，修改潜变量分布是更加直接且容易的方案。值得一提的是，这种方案基于口令集的弱局部相关性而不是神经网络本身。

这里修改P(z)的想法基于如下的假设：当我们增加更多的权重给已经猜测成功的口令，也能增加该口令附近口令的破解概率。

更加广泛地说，DPG能够潜在地适应非常独特的口令分布。只要生成器生成某条口令的概率非0，正确猜测的反馈能够不断更新潜分布的参数，使结果尽可能趋近于目标口令分布。

2）DPG实现原理

![image-20201122204730078](assets\image-20201122204730078.png)

DPG的核心在于更新潜变量分布p(z)。

makeLatentDistribution函数根据成功猜测的口令集Zj返回口令集的潜分布。通过最大似然估计方法，我们能够得最大化拟合成功猜测口令的分布。

![image-20201122210943738](assets\image-20201122210943738.png)

作为模拟口令生成的自然过程，潜空间的分布依然使用各项同性的高斯分布来表示。

![image-20201122210952396](assets\image-20201122210952396.png)

>各向同性的高斯分布（球形高斯分布）指的是各个方向方差都一样的多维高斯分布，协方差为正实数与identity matrix相乘。
>
>因为高斯的circular symmetry，只需要让每个轴上的长度一样就能得到各向同性，也就是说分布密度值仅跟点到均值距离相关，而不和方向有关。
>
>**各向同性的高斯每个维度之间也是互相独立的，因此密度方程可以写成几个1维度高斯乘积形式**。

DPG的实现基于GAN生成模型，原因是GAN模型的实际表现更加出色。

DPG的参数：

+ 热启动参数alpha。DPG在一开始的行为与PassGAN相同，直到DPG破解的口令达到一定的数量（超参数alpha）。达到热启动参数后，DPG开始调整潜分布。不在一开始就调整潜分布的原因是由于一开始已破解的口令数量较少，潜空间的更新将会局限于很小的一片空间内。等待足够的数量的破解口令后，更新潜空间将扩大匹配口令的精确度，更能接近待破解口令的分布。
+ 高斯方差的统一方差。在猜测的场景下，方差确定我们能够在距离已猜测口令多远的位置采样。越大的方差将增加口令猜测空间的范围；越小的方差将使口令猜测更加集中和有针对性。

### C. DPG的评估

![image-20201122213910382](assets\image-20201122213910382.png)

DPG与主流的口令猜测方法都进行了比较，也与所有口令猜测方法的整合策略min-auto进行比较。

+ 猜测英文数据集。DPG的效率低于FLA和PCFG。但是组合了DPG的min-auto的效率将大于无DPG的min-auto，说明DPG在原来方法的基础上能够猜测更多的口令。
+ 猜测中文数据集。中文口令数据集与训练使用的英文口令数据集有巨大差异，因此其他方法的猜测效率都相对变低。由于能够适应被猜测口令集的分布，DPG比其他根据猜测到更多的口令。
+ 猜测更加特殊的数据集。Zomato的数据集中包含约等于40%的特殊口令：随机由6位数字字符构成的口令。在这个场景下，DPG比原来的PassGAN提升了5倍猜测能力。

